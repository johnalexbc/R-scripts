---
title: "Inference vs. Prediction"
author: "Tobias RÃ¼ttenauer"
date: "November 2025"
format:
  html:
    highlight-style: github
bibliography: references.bib    
---

# Machine Learning - General Idea

We start with the usual pacakges.

```{r, include = T}
library(tidyverse)
library(haven) 
library(labelled) 
library(ggplot2)
```

## Loading the data

```{r}
#| label: data
#| echo: true

negative_to_na <- function(x){
  na_range(x) <- c(-Inf, -1)
  user_na_to_na(x)
}

w1 <- read_dta("a_indresp.dta",
               col_select = c(pidp, a_sex, a_age_dv, a_fimnlabgrs_dv,
                              a_hhsize, a_qfhigh_dv, a_jbhas)) %>% 
  mutate(
    across(where(is.labelled), negative_to_na),
    female = a_sex - 1,
    age = as.double(a_age_dv),
    gross_labinc = as.double(a_fimnlabgrs_dv)/1000,
    hhsize = as.double(a_hhsize),
    educ = a_qfhigh_dv,
    work = 2 - as.double(a_jbhas)
  )

w1 %>%
  filter(work == 1, 
         age > 25, 
         age < 67,
         female == 0) -> w1_filtered

```

## Training and test data

We split the data into two parts. Make sure to set a random seed. This ensures that your analysis is replicable.

```{r}
# Using random numbers
set.seed(123)  # for reproducibility

# Get random indices of 70% of data
index_1 <- sample(1:nrow(w1_filtered), round(nrow(w1_filtered) * 0.7))

# Split in test and train data
train_1 <- w1_filtered[index_1, ]
test_1  <- w1_filtered[-index_1, ]
```

In the following, we will only use the train_1 data and ignore the test_1 data, as this will act as our hold-out data to test out-of-sample prediction performance.

## Predictive performance within sample

Let's compare two different approaches. We estimate a linear model, one with a quadratic term, and one with dummy coding.

```{r}
#| label: mod0
#| echo: true

# Model 0
mod0.lm <- lm(gross_labinc ~ age, 
              data = train_1)
summary(mod0.lm)

# Model 1
mod1.lm <- lm(gross_labinc ~ age + I(age^2), 
              data = train_1)
summary(mod1.lm)

# Model 2
mod2.lm <- lm(gross_labinc ~ as.factor(age), 
              data = train_1)

summary(mod2.lm)
```

In terms of predictive performance, we can compare the RMSE - note that this is the within-sample performance.

```{r rmse}
rmse <- function(mod, newdata = NULL){
  if(is.null(newdata)){
    actual <- mod$model[, 1]
    predicted <- predict(mod)
    sqrt(mean((actual - predicted)^2))
  }else{
    outcomename <- all.vars(formula(mod0.lm))[1]
    vars <- all.vars(formula(mod0.lm))[1]
    actual <- newdata[, outcomename, drop = TRUE]
    predicted <- predict(mod, newdata = newdata)
    sqrt(mean((actual - predicted)^2))
  }

}

mod.perf <- list(
  linear = rmse(mod0.lm),
  quadratic = rmse(mod1.lm),
  dummy = rmse(mod2.lm)
)

mod.perf
```

Remember that we're trying to minimize the RMSE. Unsurprisingly, the dummy model has the best in-sample performance. However, keep in mind that we may be overfitting if we use the in-sample predictive performance as our main criteria for model selection.

## Predictive performance out-of-sample

Let's have a look at how well the three models predict the hold-out-sample.

```{r rmse-out-of-sample}

test_1.tmp <- test_1 %>% 
  select(, c(gross_labinc, age)) %>% 
  filter(complete.cases(.))

mod.perf2 <- list(
  linear = rmse(mod0.lm, newdata = test_1.tmp),
  quadratic = rmse(mod1.lm, newdata = test_1.tmp),
  dummy = rmse(mod2.lm, newdata = test_1.tmp)
)

mod.perf2
```

**The results have changed**: in terms of out-of-sample prediction, the quadratic function seems to do a better job than the more flexible dummy function. This means that the dummy function is over-fitting the parameters to our test data - thus leading to higher bias in the hold-out-sample.

## Cross-validation

In the above, we have used the hold-out-sample to check out-of-sample performance. Often, we want to check out-of sample performance without touching the hold-out-sample.

This can be done by cross-validation. There are many functions for this. However, we do this manually for illustration.

```{r cv}
set.seed(12345)

kfolds = 5

train_sample <- train_1 %>% 
  select(, c(gross_labinc, age)) %>% 
  filter(complete.cases(.))
  

cv_fun <- function(k = NA, data = NA, p = NA) {
  
  # Split data into folds
  train_data <- data %>%
    mutate(fold = sample(1:kfolds, size = nrow(data), replace = T))
  
  # Results object
  cv_err <- rep(0, kfolds)
  
  # Define cross-validation function
  for (i in 1:kfolds) {
    in_data <- filter(train_data, fold != i)
    out_data <- filter(train_data, fold == i)
    
    fit <- lm(gross_labinc ~ poly(age, p), data = in_data)
    res <- rmse(fit, newdata = out_data)
    
    cv_err[i] <- res
  }
  return(cv_err)
}


# Test with 1 polynom
cv_1 <- cv_fun(k = kfolds, data = train_sample, p = 1)

# Test with 2 polynom
cv_2 <- cv_fun(k = kfolds, data = train_sample, p = 2)

# Test with 3 polynom
cv_3 <- cv_fun(k = kfolds, data = train_sample, p = 3)

# Test with 5 polynom
cv_5 <- cv_fun(k = kfolds, data = train_sample, p = 5)

# Test with 10 polynom
cv_10 <- cv_fun(k = kfolds, data = train_sample, p = 10)

# Test with 20 polynom
cv_20 <- cv_fun(k = kfolds, data = train_sample, p = 20)

# Compare
res.list <- list(
  p1 = mean(cv_1),
  p2 = mean(cv_2),
  p3 = mean(cv_3),
  p5 = mean(cv_5),
  p10 = mean(cv_10),
  p20 = mean(cv_20)
)

res.list

```

We have only touched the training sample and split it into 5-folds. Each time, we use one fold as "test" and the other four folds as "training" data. What you see here is that the out-of-sample performance is first increasing (RMSE decreasing) with increasing flexibility, but then the out-of sample RMSE is getting worse again with too much flexibility.

As you know, we can write this more efficiently.

```{r cv2}
library(purrr)

set.seed(12345)

res.list <- c(1, 2, 3, 5, 10, 20) %>% 
  set_names(., .) %>% 
  map(~ cv_fun(k = kfolds, 
               data = train_sample, 
               p = .x))

res.list %>% 
  map ( ~ mean(.x))
```

## References
